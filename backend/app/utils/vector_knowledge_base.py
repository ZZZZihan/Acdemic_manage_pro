import logging
import json
import os
import pickle
import numpy as np
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
from .startup_optimizer import startup_optimizer

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class VectorKnowledgeBase:
    """
    å‘é‡çŸ¥è¯†åº“ç±»
    ä½¿ç”¨å‘é‡åµŒå…¥æ¥æ£€ç´¢ç›¸å…³æ–‡æ¡£
    """
    
    def __init__(self, knowledge_file: str = None, cache_dir: str = None):
        """
        åˆå§‹åŒ–å‘é‡çŸ¥è¯†åº“
        
        Args:
            knowledge_file: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸ºNoneåˆ™ä½¿ç”¨å†…å­˜å­˜å‚¨
            cache_dir: ç¼“å­˜ç›®å½•ï¼Œç”¨äºå­˜å‚¨é¢„è®¡ç®—çš„å‘é‡
        """
        # è®¾ç½®ç¼“å­˜ç›®å½•
        self.cache_dir = cache_dir or os.path.join(os.path.dirname(__file__), '..', 'cache')
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # å»¶è¿Ÿåˆå§‹åŒ–æ¨¡å‹
        self.model = None
        self.use_mock = False
        self._model_loaded = False
        
        # æ–‡æ¡£å’Œå‘é‡å­˜å‚¨
        self.documents = []  # å­˜å‚¨æ–‡æ¡£
        self.embeddings = []  # å­˜å‚¨æ–‡æ¡£å‘é‡
        self._embeddings_loaded = False
        
        # å¦‚æœæä¾›äº†çŸ¥è¯†åº“æ–‡ä»¶ï¼Œåˆ™åŠ è½½æ–‡æ¡£ï¼ˆä½†ä¸ç«‹å³ç”Ÿæˆå‘é‡ï¼‰
        if knowledge_file and os.path.exists(knowledge_file):
            self._load_documents_only(knowledge_file)
    
    def _load_model(self) -> None:
        """å»¶è¿ŸåŠ è½½å‘é‡æ¨¡å‹"""
        if self._model_loaded:
            return
        
        # æ£€æŸ¥æ˜¯å¦ç¦ç”¨å‘é‡æ¨¡å‹
        if startup_optimizer.should_disable_vector_model():
            logger.info("å‘é‡æ¨¡å‹å·²è¢«ç¦ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            self.model = None
            self.use_mock = True
            self._model_loaded = True
            return
            
        try:
            if not startup_optimizer.is_verbose_startup():
                logger.info("æ­£åœ¨åŠ è½½å¥å‘é‡æ¨¡å‹...")
            else:
                logger.info("æ­£åœ¨åŠ è½½å¥å‘é‡æ¨¡å‹: paraphrase-multilingual-MiniLM-L12-v2")
                
            self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            self.use_mock = False
            self._model_loaded = True
            logger.info("æˆåŠŸåŠ è½½å¥å‘é‡æ¨¡å‹")
        except Exception as e:
            logger.warning(f"åŠ è½½å¥å‘é‡æ¨¡å‹å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼: {str(e)}")
            self.model = None
            self.use_mock = True
            self._model_loaded = True
    
    def _load_documents_only(self, knowledge_file: str) -> None:
        """åªåŠ è½½æ–‡æ¡£ï¼Œä¸ç”Ÿæˆå‘é‡ï¼ˆå»¶è¿Ÿåˆ°éœ€è¦æ—¶å†ç”Ÿæˆï¼‰"""
        try:
            with open(knowledge_file, 'r', encoding='utf-8') as f:
                self.documents = json.load(f)
            
            logger.info(f"å·²ä»{knowledge_file}åŠ è½½çŸ¥è¯†åº“ï¼Œå…±{len(self.documents)}ç¯‡æ–‡æ¡£")
        except Exception as e:
            logger.exception(f"åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            self.documents = []
    
    def load_knowledge(self, knowledge_file: str) -> None:
        """
        ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†åº“
        
        Args:
            knowledge_file: çŸ¥è¯†åº“æ–‡ä»¶è·¯å¾„
        """
        self._load_documents_only(knowledge_file)
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½å‘é‡
        if self.documents:
            self._load_embeddings_from_cache(knowledge_file)
    
    def _get_cache_path(self, knowledge_file: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # åŸºäºçŸ¥è¯†åº“æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´å’Œå†…å®¹ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
        import hashlib
        
        file_stat = os.stat(knowledge_file)
        cache_key = f"{knowledge_file}_{file_stat.st_mtime}_{len(self.documents)}"
        cache_hash = hashlib.md5(cache_key.encode()).hexdigest()
        
        return os.path.join(self.cache_dir, f"embeddings_{cache_hash}.pkl")
    
    def _load_embeddings_from_cache(self, knowledge_file: str) -> bool:
        """ä»ç¼“å­˜åŠ è½½å‘é‡åµŒå…¥"""
        if not startup_optimizer.should_enable_vector_cache():
            return False
            
        cache_path = self._get_cache_path(knowledge_file)
        
        try:
            if os.path.exists(cache_path):
                if startup_optimizer.is_verbose_startup():
                    logger.info(f"æ­£åœ¨ä»ç¼“å­˜åŠ è½½å‘é‡åµŒå…¥: {cache_path}")
                else:
                    logger.info("æ­£åœ¨ä»ç¼“å­˜åŠ è½½å‘é‡åµŒå…¥...")
                    
                with open(cache_path, 'rb') as f:
                    self.embeddings = pickle.load(f)
                self._embeddings_loaded = True
                logger.info(f"âœ… æˆåŠŸä»ç¼“å­˜åŠ è½½ {len(self.embeddings)} ä¸ªå‘é‡åµŒå…¥")
                return True
        except Exception as e:
            logger.warning(f"ä»ç¼“å­˜åŠ è½½å‘é‡åµŒå…¥å¤±è´¥: {str(e)}")
        
        return False
    
    def _save_embeddings_to_cache(self, knowledge_file: str) -> None:
        """ä¿å­˜å‘é‡åµŒå…¥åˆ°ç¼“å­˜"""
        if not startup_optimizer.should_enable_vector_cache():
            return
            
        if not self.embeddings or len(self.embeddings) == 0:
            return
            
        cache_path = self._get_cache_path(knowledge_file)
        
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(self.embeddings, f)
            if startup_optimizer.is_verbose_startup():
                logger.info(f"å·²ä¿å­˜å‘é‡åµŒå…¥åˆ°ç¼“å­˜: {cache_path}")
            else:
                logger.info(f"âœ… å·²ä¿å­˜ {len(self.embeddings)} ä¸ªå‘é‡åµŒå…¥åˆ°ç¼“å­˜")
        except Exception as e:
            logger.warning(f"ä¿å­˜å‘é‡åµŒå…¥åˆ°ç¼“å­˜å¤±è´¥: {str(e)}")
    
    def _ensure_embeddings_loaded(self, knowledge_file: str = None) -> None:
        """ç¡®ä¿å‘é‡åµŒå…¥å·²åŠ è½½"""
        if self._embeddings_loaded or self.use_mock or not self.documents:
            return
        
        # é¦–å…ˆå°è¯•åŠ è½½æ¨¡å‹
        self._load_model()
        
        if self.use_mock:
            return
        
        # å¦‚æœæœ‰ç¼“å­˜ï¼Œå°è¯•åŠ è½½
        if knowledge_file and self._load_embeddings_from_cache(knowledge_file):
            return
        
        # å¦åˆ™ç”Ÿæˆæ–°çš„å‘é‡åµŒå…¥
        if startup_optimizer.is_verbose_startup():
            logger.info("æ­£åœ¨ç”Ÿæˆå‘é‡åµŒå…¥...")
        else:
            logger.info("ğŸ”„ æ­£åœ¨ç”Ÿæˆå‘é‡åµŒå…¥ï¼Œè¯·ç¨å€™...")
        self._generate_embeddings()
        
        # ä¿å­˜åˆ°ç¼“å­˜
        if knowledge_file:
            self._save_embeddings_to_cache(knowledge_file)
    
    def _generate_embeddings(self) -> None:
        """ä¸ºæ‰€æœ‰æ–‡æ¡£ç”Ÿæˆå‘é‡åµŒå…¥"""
        if self.use_mock or not self.documents:
            return
        
        try:
            # æå–æ–‡æ¡£å†…å®¹
            texts = [f"{doc['title']}. {doc['content']}" for doc in self.documents]
            
            # æ‰¹é‡ç”Ÿæˆå‘é‡åµŒå…¥ï¼Œæ˜¾ç¤ºè¿›åº¦
            batch_size = startup_optimizer.get_vector_batch_size()
            all_embeddings = []
            
            if startup_optimizer.is_verbose_startup():
                logger.info(f"ä½¿ç”¨æ‰¹æ¬¡å¤§å°: {batch_size}")
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = self.model.encode(
                    batch_texts, 
                    convert_to_numpy=True,
                    show_progress_bar=startup_optimizer.is_verbose_startup()  # æ ¹æ®é…ç½®æ˜¾ç¤ºè¿›åº¦æ¡
                )
                all_embeddings.append(batch_embeddings)
            
            # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„åµŒå…¥
            if all_embeddings:
                self.embeddings = np.vstack(all_embeddings)
                self._embeddings_loaded = True
                if startup_optimizer.is_verbose_startup():
                    logger.info(f"å·²ä¸º{len(self.documents)}ç¯‡æ–‡æ¡£ç”Ÿæˆå‘é‡åµŒå…¥")
                else:
                    logger.info(f"âœ… å·²ä¸º {len(self.documents)} ç¯‡æ–‡æ¡£ç”Ÿæˆå‘é‡åµŒå…¥")
            
        except Exception as e:
            logger.exception(f"ç”Ÿæˆå‘é‡åµŒå…¥å¤±è´¥: {str(e)}")
            self.embeddings = []
    
    def add_document(self, document: Dict[str, Any]) -> bool:
        """
        æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            document: åŒ…å«titleå’Œcontentçš„æ–‡æ¡£å­—å…¸
            
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            # ç¡®ä¿æ–‡æ¡£æ ¼å¼æ­£ç¡®
            if 'title' not in document or 'content' not in document:
                logger.error("æ–‡æ¡£æ ¼å¼é”™è¯¯ï¼Œå¿…é¡»åŒ…å«titleå’Œcontentå­—æ®µ")
                return False
            
            # ä¸ºæ–‡æ¡£æ·»åŠ ID
            if 'id' not in document:
                document['id'] = f"doc{len(self.documents) + 1}"
            
            # æ·»åŠ æ–‡æ¡£
            self.documents.append(document)
            
            # å¦‚æœå·²ç»åŠ è½½äº†å‘é‡ï¼Œä¸ºæ–°æ–‡æ¡£ç”Ÿæˆå‘é‡
            if self._embeddings_loaded and not self.use_mock:
                self._load_model()  # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
                
                if not self.use_mock:
                    text = f"{document['title']}. {document['content']}"
                    embedding = self.model.encode(text, convert_to_numpy=True, show_progress_bar=False)
                    
                    if len(self.embeddings) == 0:
                        self.embeddings = np.array([embedding])
                    else:
                        self.embeddings = np.vstack([self.embeddings, embedding])
            
            return True
        except Exception as e:
            logger.exception(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False
    
    def search_documents(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """
        æœç´¢ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„æœ€ç›¸å…³æ–‡æ¡£æ•°é‡
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        # ç¡®ä¿å‘é‡åµŒå…¥å·²åŠ è½½
        self._ensure_embeddings_loaded()
        
        if self.use_mock:
            return self._mock_search(query, top_k)
        
        if not self.documents or len(self.embeddings) == 0:
            logger.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•æœç´¢")
            return []
        
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.model.encode(query, convert_to_numpy=True, show_progress_bar=False)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = np.dot(self.embeddings, query_embedding) / (
                np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)
            )
            
            # è·å–æœ€ç›¸å…³çš„æ–‡æ¡£ç´¢å¼•
            top_indices = np.argsort(-similarities)[:top_k]
            
            # è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£
            results = []
            for i in top_indices:
                if similarities[i] > 0.3:  # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
                    doc = self.documents[i].copy()
                    doc['similarity'] = float(similarities[i])
                    results.append(doc)
            
            return results
        except Exception as e:
            logger.exception(f"æœç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")
            return self._mock_search(query, top_k)  # å¤±è´¥æ—¶å›é€€åˆ°æ¨¡æ‹Ÿæœç´¢
    
    def _mock_search(self, query: str, top_k: int) -> List[Dict[str, Any]]:
        """
        æ¨¡æ‹Ÿæœç´¢åŠŸèƒ½ï¼ˆå½“å‘é‡æœç´¢ä¸å¯ç”¨æ—¶ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            æ¨¡æ‹Ÿçš„æœç´¢ç»“æœ
        """
        if not self.documents:
            return []
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        results = []
        query_lower = query.lower()
        
        for doc in self.documents:
            title_lower = doc['title'].lower()
            content_lower = doc['content'].lower()
            
            # æ£€æŸ¥æ ‡é¢˜å’Œå†…å®¹ä¸­æ˜¯å¦åŒ…å«æŸ¥è¯¢å…³é”®è¯
            if query_lower in title_lower or query_lower in content_lower:
                # è®¡ç®—ä¸€ä¸ªæ¨¡æ‹Ÿçš„ç›¸ä¼¼åº¦åˆ†æ•°
                # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
                score = 0.0
                if query_lower in title_lower:
                    score += 0.8
                if query_lower in content_lower:
                    score += 0.5
                
                doc_copy = doc.copy()
                doc_copy['similarity'] = min(score, 0.99)  # é™åˆ¶æœ€å¤§åˆ†æ•°
                results.append(doc_copy)
        
        # å¦‚æœå…³é”®è¯åŒ¹é…æ‰¾ä¸åˆ°ç»“æœï¼Œè¿”å›éšæœºæ–‡æ¡£
        if not results and self.documents:
            # é€‰æ‹©ä¸€äº›éšæœºæ–‡æ¡£
            import random
            random_docs = random.sample(
                self.documents, 
                min(top_k, len(self.documents))
            )
            for doc in random_docs:
                doc_copy = doc.copy()
                doc_copy['similarity'] = random.uniform(0.3, 0.5)  # è¾ƒä½çš„ç›¸ä¼¼åº¦
                results.append(doc_copy)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é™åˆ¶è¿”å›æ•°é‡
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:top_k]


# åˆ›å»ºé»˜è®¤çš„å‘é‡çŸ¥è¯†åº“å®ä¾‹
vector_knowledge_base = VectorKnowledgeBase()